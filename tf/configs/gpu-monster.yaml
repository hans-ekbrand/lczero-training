%YAML 1.2
---
name: 'kb1-64x6-aries'                       # ideally no spaces
gpu: 0                                 # gpu id to process on

# based on Chads recommendations, 4000 games, 250 steps per net, learning rate 0.5 until no improvement.
# I started out with 400 chunks, 25 steps per net, learning rate 0.25 and changed to 4000, 250, 0.25 at net 172
# At net 240 reduced the number of steps per net to 125 (from 250).
# First learning rate drop at step 32250, net 336. Motivated by no improvement vs external opponent in 30 nets (also policy accuracy does not seem to be improving).

dataset: 
  num_chunks: 4000                   # newest nof chunks to parse (chunks is games, for all I know).
  allow_less_chunks: true
  train_ratio: 0.90                    # trainingset ratio
  input: '/mnt/leela-training-games/aries/lc0/*/' # this is a location on the game generating client.
  ## experimental_v5_only_dataset: true # requires v5 training format support which appeared in March 22 2020.

training:
    batch_size: 1024                   # training batch size. If you have the memory, 4096 is reported to be even faster. Note the linear correlation between batch size and learning rate!
    test_steps: 25                      # eval test set values after this many steps
    train_avg_report_steps: 125        # training reports its average values after this many steps.
    total_steps: 125                # terminate after these steps
    warmup_steps: 125                  # if global step is less than this, scale the current LR by ratio of global step to this value
    # checkpoint_steps: 10000          # optional frequency for checkpointing before finish
    shuffle_size: 524288               # size of the shuffle buffer
    lr_values:                         # list of learning rates
    # if you increase the batch_size by four, then increase the learning rate be four as well!
        - 0.25
        - 0.025
        - 0.001
    lr_boundaries:                     # list of boundaries. Ignore these, change Learning Rate manually when there is no Elo-improvment using a fixed anchor, e.g. Stockfish.
        - 32250			       # starting with net 336.gz
        - 64500
    policy_loss_weight: 1.0            # weight of policy loss
    value_loss_weight: 1.0             # weight of value loss
    path: '/mnt'    # network storage dir
    moves_left_loss_weight: .01

model:
  filters: 64
  residual_blocks: 6
  se_ratio: 4

  
...
