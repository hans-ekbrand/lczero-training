%YAML 1.2
---
name: 'kb1-64x6-aries'                       # ideally no spaces
gpu: 0                                 # gpu id to process on

# based on Chads recommendations, 4000 games, 250 steps per net, learning rate 0.5 until no improvement.
# I started out with 400 chunks, 25 steps per net, learning rate 0.25 and changed to 4000, 250, 0.25 at net 172
# At net 240 reduced the number of steps per net to 125 (from 250).
# First learning rate drop at step 32250, net 338. Motivated by no improvement vs external opponent in 30 nets (also policy accuracy does not seem to be improving).
# Just before dropping the LR, I manually made a net (337) using 120000 chunks and 1250 steps, so extract as much as possible from the existing data
# After that back to 4000 chunks, 125 steps, but now at lr 0.025
# change to 8000 fresh games per net and 250 steps, starting with net 477.
# increasing window size to 16000, keeping 8000 new starting with net 489.

dataset: 
  num_chunks: 16000                   # newest nof chunks to parse (chunks is games, for all I know).
#  num_chunks: 8000                   # newest nof chunks to parse (chunks is games, for all I know).
#  num_chunks: 120000                   # newest nof chunks to parse (chunks is games, for all I know).  
  allow_less_chunks: true
  train_ratio: 0.90                    # trainingset ratio
  input: '/mnt/leela-training-games/aries/lc0/*/' # this is a location on the game generating client.
  ## experimental_v5_only_dataset: true # requires v5 training format support which appeared in March 22 2020.

training:
    batch_size: 1024                   # training batch size. Note the linear correlation between batch size and learning rate!
    test_steps: 25                      # eval test set values after this many steps
    train_avg_report_steps: 125        # training reports its average values after this many steps.
    total_steps: 250                # terminate after these steps
#    total_steps: 1250                # terminate after these steps    
    warmup_steps: 25                  # if global step is less than this, scale the current LR by ratio of global step to this value
    # checkpoint_steps: 10000          # optional frequency for checkpointing before finish
    shuffle_size: 524288               # size of the shuffle buffer
    lr_values:
# list of learning rates    
# if you increase the batch_size by four, then increase the learning rate by four as well!
#        - 0.25
#        - 0.025
        - 0.01
        - 0.001
# list of boundaries. These seems to be ignored when train.py is restarted for each net, possibly related to "Error occurred when finalizing GeneratorDataset iterator: Failed precondition: Python interpreter state is not initialized. The process may be terminated.", perhaps the iterator is forgotten?
# So, you have to change Learning Rate manually, and you should do it when there is no Elo-improvment using a fixed anchor, e.g. Stockfish.
# lr 0.025 at 32250 starting with net 336.gz
    lr_boundaries:
        - 32250
        - 64500
    policy_loss_weight: 1.0            # weight of policy loss
    value_loss_weight: 1.0             # weight of value loss
    path: '/mnt'    # network storage dir
    moves_left_loss_weight: .01

model:
  filters: 64
  residual_blocks: 6
  se_ratio: 4

  
...
